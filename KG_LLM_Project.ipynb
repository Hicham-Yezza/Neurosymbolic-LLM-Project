{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0TWmzndSgRyq259PLIUZ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hicham-Yezza/Neurosymbolic-LLM-Project/blob/main/KG_LLM_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je-eB9MLCvvR"
      },
      "outputs": [],
      "source": [
        "# KG-Enhanced Summarisation Project - Hicham Yezza 2024"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall the incompatible version of pyarrow\n",
        "!pip uninstall pyarrow -y\n",
        "\n",
        "# Install the compatible version of pyarrow\n",
        "!pip install pyarrow==14.0.1\n",
        "\n",
        "# Restart the runtime (you will need to manually restart the runtime after this step)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Vqa8l5bOqqwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export LC_ALL=C.UTF-8\n",
        "!export LANG=C.UTF-8"
      ],
      "metadata": {
        "id": "fUb0e_gPmM5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Select device based on GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Ensure mixed precision support\n",
        "torch.backends.cuda.matmul.allow_tf32 = True"
      ],
      "metadata": {
        "id": "fbMJKBw-g-vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "NO75K42xGDrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all necessary libraries in one line, avoiding redundancy\n",
        "!pip install wandb rouge_score sacrebleu bert-score spacy networkx datasets pandas tqdm transformers nltk torch evaluate node2vec sentence_transformers\n",
        "# Download SpaCy models (both transformer-based and small model)\n",
        "!python -m spacy download en_core_web_trf\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VgNRMAzL1OC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple, Dict\n",
        "from transformers import logging\n",
        "\n",
        "# Third-Party Library Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "from tqdm import tqdm, notebook  # For progress bars\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# SpaCy for NLP\n",
        "import spacy\n",
        "\n",
        "# Hugging Face Transformers\n",
        "from transformers import (\n",
        "    BartTokenizer, BartForConditionalGeneration, AdamW, get_scheduler, BartConfig, pipeline\n",
        ")\n",
        "\n",
        "# Datasets, Evaluation, and Metrics\n",
        "from datasets import load_dataset\n",
        "from evaluate import load as load_metric\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "import bert_score\n",
        "\n",
        "# Knowledge Graph and Node Embedding\n",
        "from node2vec import Node2Vec\n",
        "\n",
        "# Weights and Biases for Experiment Tracking\n",
        "import wandb\n",
        "\n",
        "# Sentence Transformers for Embeddings\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "Dx1hF8d6DF6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "timestamp = int(time.time())\n",
        "warnings.filterwarnings(\"ignore\", message=\"Some weights of RobertaModel were not initialized from the model checkpoint\")\n",
        "\n",
        "# Set logging level to ERROR to suppress warnings\n",
        "logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "LbiT96ngq0hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the SpaCy model for NER (Transformer-based for higher accuracy)\n",
        "nlp = spacy.load(\"en_core_web_trf\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0PmAdx21r0mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entity extraction, relation extraction, KG generation"
      ],
      "metadata": {
        "id": "UQE5tmGtDT4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define KnowledgeGraphExtractor class with improved entity extraction and relation extraction\n",
        "class KnowledgeGraphExtractor:\n",
        "    def __init__(self, use_trf_model=True):\n",
        "        if use_trf_model:\n",
        "            spacy.require_gpu()  # Ensure the GPU is used if available\n",
        "            self.nlp = spacy.load(\"en_core_web_trf\")\n",
        "        else:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.matcher = spacy.matcher.Matcher(self.nlp.vocab)\n",
        "        self._add_patterns()\n",
        "\n",
        "    def _add_patterns(self):\n",
        "        # Add common subject-verb-object patterns to the matcher\n",
        "        patterns = [\n",
        "            [{'DEP': 'nsubj'}, {'DEP': 'ROOT'}, {'DEP': 'dobj'}],\n",
        "            [{'DEP': 'nsubj'}, {'DEP': 'ROOT'}, {'DEP': 'prep'}, {'DEP': 'pobj'}],\n",
        "            [{'DEP': 'nsubj'}, {'DEP': 'ROOT'}, {'DEP': 'attr'}],\n",
        "            [{'DEP': 'nsubjpass'}, {'DEP': 'ROOT'}, {'DEP': 'pobj'}],  # For passive voice\n",
        "        ]\n",
        "        for pattern in patterns:\n",
        "            self.matcher.add(\"SVO\", [pattern])\n",
        "\n",
        "    def _merge_similar_entities(self, entities):\n",
        "        # Helper function to merge similar entities like \"John Terry\" and \"Terry\"\n",
        "        merged_entities = {}\n",
        "        for entity, label in entities:\n",
        "            key = re.sub(r\"\\b(the|a|an)\\b\", \"\", entity.lower()).strip()  # Remove articles\n",
        "            if key not in merged_entities:\n",
        "                merged_entities[key] = (entity, label)\n",
        "        return list(merged_entities.values())\n",
        "\n",
        "    def _filter_low_value_entities(self, entities):\n",
        "        # Filter out low-value entities like generic noun chunks (\"his wife\", \"this\")\n",
        "        low_value_terms = {\"this\", \"we\", \"his\", \"her\", \"it\", \"they\", \"he\", \"she\"}\n",
        "        return [(entity, label) for entity, label in entities if entity.lower() not in low_value_terms]\n",
        "\n",
        "    def clean_entities(self, entities):\n",
        "        # Additional entity cleaning logic to refine and filter entities\n",
        "        cleaned_entities = []\n",
        "        seen = set()\n",
        "\n",
        "        for entity, label in entities:\n",
        "            if entity.lower() not in seen:\n",
        "                seen.add(entity.lower())\n",
        "                if len(entity) > 2 and not entity.isdigit():  # Avoid single-character entities and pure numbers\n",
        "                    cleaned_entities.append((entity, label))\n",
        "\n",
        "        return cleaned_entities\n",
        "\n",
        "    def extract_entities(self, doc):\n",
        "        # Extract named entities and noun chunks\n",
        "        entities = set()\n",
        "        logging.info(f\"Document text: {doc.text[:100]}\")  # Log first 100 characters of the document\n",
        "        logging.info(f\"Detected entities: {[ent.text for ent in doc.ents]}\")\n",
        "\n",
        "        # Add named entities to the set\n",
        "        for ent in doc.ents:\n",
        "            entities.add((ent.text.strip(), ent.label_))\n",
        "\n",
        "        # Also consider noun chunks as potential entities\n",
        "        for chunk in doc.noun_chunks:\n",
        "            entities.add((chunk.text.strip(), \"NOUN_CHUNK\"))\n",
        "\n",
        "        # Merge similar entities, filter out low-value ones, and clean the entities\n",
        "        entities = self._merge_similar_entities(entities)\n",
        "        entities = self._filter_low_value_entities(entities)\n",
        "        entities = self.clean_entities(entities)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_relations(self, doc):\n",
        "        # Extract relations using pattern matching and subject-object dependencies\n",
        "        matches = self.matcher(doc)\n",
        "        relations = []\n",
        "\n",
        "        # Extract SVO triples using pattern matches\n",
        "        for match_id, start, end in matches:\n",
        "            span = doc[start:end]\n",
        "            if len(span) >= 3:\n",
        "                subj, verb, obj = span[0].text.strip(), span[1].text, span[-1].text.strip()\n",
        "                relations.append((subj, verb, obj))\n",
        "\n",
        "        # Extract additional relations using dependency parsing\n",
        "        for token in doc:\n",
        "            if token.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "                for child in token.head.children:\n",
        "                    if child.dep_ == \"dobj\":\n",
        "                        relations.append((token.text.strip(), token.head.text, child.text.strip()))\n",
        "                    elif child.dep_ == \"prep\" and child.children:\n",
        "                        for pobj in child.children:\n",
        "                            if pobj.dep_ == \"pobj\":\n",
        "                                relations.append((token.text.strip(), token.head.text + \" \" + child.text, pobj.text.strip()))\n",
        "\n",
        "        # Filter out redundant or trivial relations\n",
        "        relations = self._filter_trivial_relations(relations)\n",
        "        return relations\n",
        "\n",
        "    def _filter_trivial_relations(self, relations):\n",
        "        # Filter out low-value or redundant relations\n",
        "        low_value_verbs = {\"is\", \"are\", \"was\", \"were\", \"have\", \"has\", \"had\", \"do\", \"did\"}\n",
        "        filtered_relations = [(subj, verb, obj) for subj, verb, obj in relations if verb.lower() not in low_value_verbs]\n",
        "        return filtered_relations\n",
        "\n",
        "# Define KnowledgeGraphGenerator class for generating and post-processing the KG\n",
        "class KnowledgeGraphGenerator:\n",
        "    def __init__(self, use_trf_model=True):\n",
        "        self.extractor = KnowledgeGraphExtractor(use_trf_model)\n",
        "\n",
        "    def create_graph(self, text):\n",
        "        # Process text and extract entities and relations\n",
        "        doc = self.extractor.nlp(text)\n",
        "        entities = self.extractor.extract_entities(doc)\n",
        "        relations = self.extractor.extract_relations(doc)\n",
        "\n",
        "        # Log extracted entities and relations for debugging, limiting the output size\n",
        "        logging.info(f\"Extracted entities: {entities[:5]}...\")  # Limit entity log output to first 5 items\n",
        "        logging.info(f\"Extracted relations: {relations[:5]}...\")  # Limit relation log output to first 5 items\n",
        "\n",
        "        # Create the knowledge graph using NetworkX\n",
        "        G = nx.Graph()\n",
        "\n",
        "        # Add nodes for each entity\n",
        "        for entity, label in entities:\n",
        "            G.add_node(entity, label=label)\n",
        "\n",
        "        # Add edges for each relation\n",
        "        for subj, pred, obj in relations:\n",
        "            if G.has_node(subj) and G.has_node(obj):\n",
        "                G.add_edge(subj, obj, relation=pred)\n",
        "\n",
        "        return G\n",
        "\n",
        "    def clean_knowledge_graph(self, G):\n",
        "        # Remove isolated or irrelevant nodes (avoid removing too many)\n",
        "        isolated_nodes = [node for node, degree in G.degree() if degree == 0]\n",
        "        G.remove_nodes_from(isolated_nodes)\n",
        "\n",
        "        # Dynamically remove low-value nodes that add little to the graph\n",
        "        low_value_nodes = [node for node in G.nodes if re.match(r\"\\b(this|that|we|he|she|they)\\b\", node, re.I)]\n",
        "        G.remove_nodes_from(low_value_nodes)\n",
        "\n",
        "        return G"
      ],
      "metadata": {
        "id": "CFs2FV931rsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wandb setup\n",
        "wandb.login()\n",
        "\n",
        "# for wandb sweep hp tuning\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',  # Bayesian search\n",
        "    'metric': {'name': 'rouge', 'goal': 'maximize'},  # Optimize based on Rouge score\n",
        "    'parameters': {\n",
        "        'max_length': {'values': [100, 150, 200]},\n",
        "        'min_length': {'values': [40, 50, 60]},\n",
        "        'num_beams': {'values': [4, 6, 8]},\n",
        "        'graph_attention_weight': {'values': [0.3, 0.5, 0.7]},  # For graph-aware models\n",
        "        'walk_length': {'values': [10, 30, 50]},  # Node2Vec\n",
        "        'num_walks': {'values': [100, 200, 300]},  # Node2Vec\n",
        "        'dimensions': {'values': [64, 128]},  # Node2Vec\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cfzDKcxQ1OKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the four summarizer classes"
      ],
      "metadata": {
        "id": "g4vtnDPa1mn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseBARTSummarizer: ## base model\n",
        "    def __init__(self, model_name: str = 'facebook/bart-large'):\n",
        "        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.model.to(device)\n",
        "\n",
        "    def summarize(self, text: str, max_length: int = 150, min_length: int = 40) -> str:\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "        summary_ids = self.model.generate(inputs['input_ids'], max_length=max_length, min_length=min_length, num_beams=4)\n",
        "        return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "HfC_AdmhIbau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KGEnhancedSummarizer: # KG-Enhanced Summarizer Base Class\n",
        "    def __init__(self, model_name: str = 'facebook/bart-large'):\n",
        "        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.kg_generator = KnowledgeGraphGenerator()\n",
        "\n",
        "    def summarize(self, text: str, max_length: int = 150, min_length: int = 40) -> str:\n",
        "        raise NotImplementedError(\"This method should be implemented by subclasses\")\n",
        "\n",
        "    def save_model(self, path: str):\n",
        "        self.model.save_pretrained(path)\n",
        "        self.tokenizer.save_pretrained(path)"
      ],
      "metadata": {
        "id": "u6LWf9Gh1mlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KGEnhancedInputSummarizer(KGEnhancedSummarizer):  # With structured prompting\n",
        "    def summarize(self, text: str, max_length: int = 150, min_length: int = 40) -> str:\n",
        "        # Step 1: Generate the knowledge graph from the input text\n",
        "        G = self.kg_generator.create_graph(text)\n",
        "\n",
        "        # Step 2: Construct the knowledge graph representation\n",
        "        entities = \", \".join([f\"{node}\" for node in G.nodes()])\n",
        "        relations = \", \".join([f\"{u}-{G[u][v].get('relation', 'related')}-{v}\" for u, v in G.edges()])\n",
        "\n",
        "        # Step 3: Create structured prompt for the LLM\n",
        "        structured_prompt = f\"\"\"\n",
        "        Knowledge Graph Information:\n",
        "        - Entities: {entities}\n",
        "        - Relations: {relations}\n",
        "\n",
        "        Article:\n",
        "        {text}\n",
        "\n",
        "        Task: Summarize the article considering the knowledge graph information above, focusing on how the entities and relationships influence the article's content.\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 4: Tokenize the structured prompt for input to the model\n",
        "        inputs = self.tokenizer(structured_prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "\n",
        "        # Step 5: Generate the summary using the model\n",
        "        summary_ids = self.model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=max_length,\n",
        "            min_length=min_length,\n",
        "            num_beams=4\n",
        "        )\n",
        "\n",
        "        # Step 6: Decode and return the generated summary\n",
        "        return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "2ClvNztn1fB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphAwareAttentionSummarizer(KGEnhancedSummarizer): ## KG-Enhanced Graph-Aware summarizer\n",
        "    \"\"\"\n",
        "    This class enhances BART-based summarization with graph-aware attention,\n",
        "    including second-order neighbors in the attention mask.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name='facebook/bart-large', device='cuda'):\n",
        "        super().__init__()\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = BartForConditionalGenerationWithGraphAttention.from_pretrained(model_name)\n",
        "        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def _create_graph_attention_mask(self, graph_data, input_length):\n",
        "        \"\"\"\n",
        "        Generates an attention mask based on the provided knowledge graph data,\n",
        "        including second-order neighbors.\n",
        "\n",
        "        Args:\n",
        "            graph_data: The graph structure containing relationships between nodes (entities).\n",
        "            input_length: The length of the input sequence for the summarization model.\n",
        "\n",
        "        Returns:\n",
        "            A graph-aware attention mask of shape (input_length, input_length).\n",
        "        \"\"\"\n",
        "        # Assume graph_data contains adjacency matrix\n",
        "        adjacency_matrix = graph_data.get('adjacency_matrix', None)\n",
        "\n",
        "        if adjacency_matrix is None or adjacency_matrix.shape[0] != input_length:\n",
        "            logging.warning(\"Graph data missing or mismatched with input length.\")\n",
        "            return torch.ones((input_length, input_length), device=self.device)  # Default to all 1s (no extra attention)\n",
        "\n",
        "        # Normalize adjacency matrix to be suitable for attention\n",
        "        normalized_adjacency_matrix = F.softmax(adjacency_matrix.float(), dim=-1)\n",
        "\n",
        "        # Ensure the matrix is symmetric for bidirectional attention\n",
        "        graph_attention_mask = 0.5 * (normalized_adjacency_matrix + normalized_adjacency_matrix.T)\n",
        "\n",
        "        # Add second-order neighbors by multiplying the mask with itself\n",
        "        second_order_neighbors = torch.mm(graph_attention_mask, graph_attention_mask)\n",
        "        graph_attention_mask += second_order_neighbors\n",
        "\n",
        "        # Normalize the final mask\n",
        "        graph_attention_mask = graph_attention_mask / (graph_attention_mask.sum(dim=-1, keepdim=True) + 1e-9)\n",
        "\n",
        "        return graph_attention_mask\n",
        "\n",
        "    def summarize(self, input_text, graph_data=None, max_length=142, min_length=56):\n",
        "        \"\"\"\n",
        "        Summarizes the given input text with optional graph-aware attention.\n",
        "\n",
        "        Args:\n",
        "            input_text: The input text to summarize.\n",
        "            graph_data: Knowledge graph data, including adjacency matrix and entities.\n",
        "            max_length: Maximum length of the generated summary.\n",
        "            min_length: Minimum length of the generated summary.\n",
        "\n",
        "        Returns:\n",
        "            The generated summary text.\n",
        "        \"\"\"\n",
        "        inputs = self.tokenizer(input_text, return_tensors='pt', max_length=1024, truncation=True)\n",
        "        input_ids = inputs['input_ids'].to(self.device)\n",
        "        attention_mask = inputs['attention_mask'].to(self.device)\n",
        "\n",
        "        # If graph data is provided, create the graph attention mask\n",
        "        graph_attention_mask = self._create_graph_attention_mask(graph_data, input_ids.shape[1]) if graph_data else None\n",
        "\n",
        "        # Perform summarization with the model\n",
        "        summary_ids = self.model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            graph_attention_mask=graph_attention_mask,  # Pass the graph-aware mask\n",
        "            max_length=max_length,\n",
        "            min_length=min_length,\n",
        "            num_beams=4,\n",
        "            length_penalty=2.0,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        # Decode and return the generated summary\n",
        "        return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def visualize_graph_attention(self, attention_data, graph_data):\n",
        "        \"\"\"\n",
        "        Visualizes the graph attention for debugging purposes.\n",
        "\n",
        "        Args:\n",
        "            attention_data: Attention weights from the model.\n",
        "            graph_data: The input knowledge graph structure.\n",
        "        \"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "        import networkx as nx\n",
        "\n",
        "        adjacency_matrix = graph_data.get('adjacency_matrix', None)\n",
        "        if adjacency_matrix is None:\n",
        "            logging.error(\"No graph data to visualize.\")\n",
        "            return\n",
        "\n",
        "        # Create a graph plot with the attention weights as edge attributes\n",
        "        graph = nx.from_numpy_matrix(adjacency_matrix.cpu().numpy())\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        pos = nx.spring_layout(graph)\n",
        "        nx.draw(graph, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\", node_size=500, font_size=10)\n",
        "\n",
        "        plt.title(\"Graph Attention Visualization\")\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "KR7gAacW35Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KGConsistencyCheckingSummarizer(KGEnhancedSummarizer):\n",
        "    def __init__(self, model_name: str = 'facebook/bart-large', threshold: float = 0.3):\n",
        "        super().__init__(model_name)\n",
        "        self.consistency_checker = ImprovedConsistencyChecker()\n",
        "        self.threshold = threshold  # Allowing a flexible threshold\n",
        "\n",
        "    def summarize(self, text: str, max_length: int = 150, min_length: int = 40) -> str:\n",
        "        # Step 1: Generate the knowledge graph for the original article\n",
        "        G_article = self.kg_generator.create_graph(text)\n",
        "        if not G_article.nodes():\n",
        "            logging.error(f\"No nodes generated in the KG for the article.\")\n",
        "            return \"\"\n",
        "\n",
        "        # Step 2: Generate the initial summary\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "        summary_ids = self.model.generate(inputs['input_ids'], max_length=max_length, min_length=min_length, num_beams=4)\n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Step 3: Generate the graph for the generated summary\n",
        "        G_summary = self.kg_generator.create_graph(summary)\n",
        "        if not G_summary.nodes():\n",
        "            logging.error(f\"No nodes generated in the KG for the summary.\")\n",
        "            return summary\n",
        "\n",
        "        # Step 4: Calculate the consistency score between the article and the summary\n",
        "        consistency_score = self.consistency_checker.calculate_consistency(G_article, G_summary)\n",
        "        logging.info(f\"Initial Consistency Score: {consistency_score}\")\n",
        "\n",
        "        # Step 5: If the consistency score is below the threshold, regenerate the summary\n",
        "        retry_count = 0\n",
        "        max_retries = 5  # Avoid excessive retries\n",
        "        while consistency_score < self.threshold and retry_count < max_retries:\n",
        "            logging.info(\"Consistency score below threshold, regenerating summary with updated parameters.\")\n",
        "            summary_ids = self.model.generate(inputs['input_ids'], max_length=max_length, min_length=min_length, num_beams=6, repetition_penalty=2.0)\n",
        "            summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "            # Step 6: Recalculate consistency for the new summary\n",
        "            G_summary = self.kg_generator.create_graph(summary)\n",
        "            consistency_score = self.consistency_checker.calculate_consistency(G_article, G_summary)\n",
        "            logging.info(f\"Revised Consistency Score: {consistency_score}\")\n",
        "            retry_count += 1\n",
        "\n",
        "        # Return the final summary (either the original or the regenerated one)\n",
        "        return summary"
      ],
      "metadata": {
        "id": "mncYxcjO34HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedConsistencyChecker:\n",
        "    def __init__(self):\n",
        "        # Load a pre-trained sentence transformer model\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def calculate_consistency(self, G1: nx.Graph, G2: nx.Graph) -> float:\n",
        "        # Extract nodes from both graphs\n",
        "        nodes1 = list(G1.nodes())\n",
        "        nodes2 = list(G2.nodes())\n",
        "\n",
        "        # If both graphs have no nodes, consider them similar (consistent)\n",
        "        if not nodes1 or not nodes2:\n",
        "            logging.warning(\"One or both graphs have no nodes.\")\n",
        "            return 0.0\n",
        "\n",
        "        # Generate node embeddings using the pre-trained model\n",
        "        try:\n",
        "            embeddings1 = self.model.encode(nodes1)\n",
        "            embeddings2 = self.model.encode(nodes2)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error encoding nodes: {e}\")\n",
        "            return 0.0  # Return default score in case of failure\n",
        "\n",
        "        # Calculate cosine similarity for the node embeddings\n",
        "        similarity_matrix = cosine_similarity(embeddings1, embeddings2)\n",
        "        node_similarity = similarity_matrix.max(axis=1).mean()\n",
        "\n",
        "        # Extract edges and relations from both graphs\n",
        "        edges1 = list(G1.edges(data=True))\n",
        "        edges2 = list(G2.edges(data=True))\n",
        "\n",
        "        # Handle cases where one or both graphs have no edges\n",
        "        if not edges1 and not edges2:\n",
        "            edge_similarity = 1.0  # Both graphs have no edges, consider them similar\n",
        "        elif not edges1 or not edges2:\n",
        "            edge_similarity = 0.0  # One graph has edges, the other doesn't\n",
        "        else:\n",
        "            # Extract relation labels and compute embeddings for the relations\n",
        "            edge_relations1 = [edge_data.get('relation', '') for _, _, edge_data in edges1]\n",
        "            edge_relations2 = [edge_data.get('relation', '') for _, _, edge_data in edges2]\n",
        "\n",
        "            try:\n",
        "                relation_embeddings1 = self.model.encode(edge_relations1)\n",
        "                relation_embeddings2 = self.model.encode(edge_relations2)\n",
        "                edge_similarity_matrix = cosine_similarity(relation_embeddings1, relation_embeddings2)\n",
        "                edge_similarity = edge_similarity_matrix.max(axis=1).mean()\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error encoding edges: {e}\")\n",
        "                edge_similarity = 0.0  # Return default score in case of failure\n",
        "\n",
        "        # Return the final consistency score based on node and edge similarities\n",
        "        return (node_similarity + edge_similarity) / 2\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "z1VErHWMeQwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom BART models"
      ],
      "metadata": {
        "id": "CHwvJJR0124C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BartForConditionalGenerationWithKGEmbeddings(BartForConditionalGeneration):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.kg_embedding_projection = torch.nn.Linear(config.d_model + 64, config.d_model)\n",
        "        self.kg_embedding_projection.to(device)  # Move to the correct device\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, kg_embeddings=None, **kwargs):\n",
        "        if kg_embeddings is not None:\n",
        "            inputs_embeds = self.model.encoder.embed_tokens(input_ids) * self.model.encoder.embed_scale\n",
        "            inputs_embeds = torch.cat([inputs_embeds, kg_embeddings], dim=-1)\n",
        "            inputs_embeds = self.kg_embedding_projection(inputs_embeds)\n",
        "            encoder_outputs = self.model.encoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, **kwargs)\n",
        "            return super().forward(encoder_outputs=encoder_outputs, attention_mask=attention_mask, **kwargs)\n",
        "        else:\n",
        "            return super().forward(input_ids=input_ids, attention_mask=attention_mask, **kwargs)"
      ],
      "metadata": {
        "id": "J1QxJu4yAF1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BartForConditionalGenerationWithGraphAttention(BartForConditionalGeneration):\n",
        "    def forward(self, input_ids, attention_mask=None, graph_attention_mask=None, **kwargs):\n",
        "        \"\"\"\n",
        "        The forward method now combines the standard attention mask with the graph attention mask.\n",
        "        The graph attention mask adjusts attention based on the structure of the knowledge graph.\n",
        "        \"\"\"\n",
        "        if graph_attention_mask is not None:\n",
        "            # Combine normal attention with graph-based attention using a scaling factor\n",
        "            combined_attention = attention_mask.unsqueeze(1) * (1 + self.graph_attention_weight * graph_attention_mask.unsqueeze(0))\n",
        "            combined_attention = combined_attention / (combined_attention.sum(dim=-1, keepdim=True) + 1e-9)\n",
        "        else:\n",
        "            combined_attention = attention_mask\n",
        "\n",
        "        # Pass the combined attention through the BART model\n",
        "        return super().forward(input_ids, attention_mask=combined_attention, **kwargs)"
      ],
      "metadata": {
        "id": "KHrFqOy-oDJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "meLfJL9F193O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SummarizerEvaluator:\n",
        "    def __init__(self):\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        self.meteor = evaluate.load(\"meteor\")\n",
        "        self.smoothie = SmoothingFunction().method1\n",
        "\n",
        "    def evaluate(self, reference: str, candidate: str) -> Dict[str, float]:\n",
        "        rouge_scores = self.rouge_scorer.score(reference, candidate)\n",
        "        bleu_score = sentence_bleu([reference.split()], candidate.split(), smoothing_function=self.smoothie)\n",
        "        meteor_score = self.meteor.compute(predictions=[candidate], references=[reference])['meteor']\n",
        "\n",
        "        # Calculate BERTScore\n",
        "        P, R, F1 = bert_score.score([candidate], [reference], lang=\"en\", verbose=False)\n",
        "\n",
        "        return {\n",
        "            \"rouge1\": rouge_scores['rouge1'].fmeasure,\n",
        "            \"rouge2\": rouge_scores['rouge2'].fmeasure,\n",
        "            \"rougeL\": rouge_scores['rougeL'].fmeasure,\n",
        "            \"bleu\": bleu_score,\n",
        "            \"meteor\": meteor_score,\n",
        "            \"bertscore_precision\": P.item(),\n",
        "            \"bertscore_recall\": R.item(),\n",
        "            \"bertscore_f1\": F1.item()\n",
        "        }"
      ],
      "metadata": {
        "id": "hxfixNH82C0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison"
      ],
      "metadata": {
        "id": "m31hhgxr2C3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_summarizers(dataset, summarizers: Dict[str, KGEnhancedSummarizer], num_samples: int = 50):\n",
        "    evaluator = SummarizerEvaluator()\n",
        "    results = {name: [] for name in summarizers.keys()}\n",
        "    example_summaries = {name: [] for name in summarizers.keys()}\n",
        "\n",
        "    for item in tqdm(dataset.select(range(num_samples)), desc=\"Processing articles\"):\n",
        "        article = item['article']\n",
        "        reference_summary = item['highlights']\n",
        "\n",
        "        for name, summarizer in summarizers.items():\n",
        "            try:\n",
        "                generated_summary = summarizer.summarize(article)\n",
        "                scores = evaluator.evaluate(reference_summary, generated_summary)\n",
        "                results[name].append(scores)\n",
        "\n",
        "                # Save example summaries (let's save the first 5)\n",
        "                if len(example_summaries[name]) < 5:\n",
        "                    example_summaries[name].append({\n",
        "                        'article': article,\n",
        "                        'reference': reference_summary,\n",
        "                        'generated': generated_summary\n",
        "                    })\n",
        "\n",
        "                # Log individual sample results to wandb\n",
        "                wandb.log({f\"{name}_{metric}\": score for metric, score in scores.items()})\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {name} summarizer: {str(e)}\")\n",
        "                results[name].append({metric: float('nan') for metric in evaluator.evaluate(\"\", \"\").keys()})\n",
        "\n",
        "    # Calculate average scores\n",
        "    avg_results = {}\n",
        "    for name, scores_list in results.items():\n",
        "        avg_scores = {metric: np.mean([s[metric] for s in scores_list]) for metric in scores_list[0].keys()}\n",
        "        avg_results[name] = avg_scores\n",
        "\n",
        "        # Log average scores to wandb\n",
        "        wandb.log({f\"avg_{name}_{metric}\": score for metric, score in avg_scores.items()})\n",
        "\n",
        "    return results, avg_results, example_summaries"
      ],
      "metadata": {
        "id": "45IhNk6v2IXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution"
      ],
      "metadata": {
        "id": "Xcf3yeG02MVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a new wandb run\n",
        "run = wandb.init(project=\"kg-enhanced-summarization\", name=f\"comparison-run-{timestamp}\") # a unique timestamp identifier for each run"
      ],
      "metadata": {
        "id": "vPruiMBWsVic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset loading\n",
        "dataset = load_dataset('cnn_dailymail', '3.0.0', split='train[:100]')"
      ],
      "metadata": {
        "id": "F8wcuxGCsVp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "# Debug: Print dataset info\n",
        "print(\"Dataset info:\")\n",
        "print(dataset)\n",
        "print(\"\\nDataset features:\")\n",
        "print(dataset.features)\n",
        "print(\"\\nFirst item in dataset:\")\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "8bPujZYV5TYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizers = {\n",
        "    \"Base BART\": BaseBARTSummarizer(),\n",
        "    \"KG-Enhanced Input\": KGEnhancedInputSummarizer(),\n",
        "    \"Graph-Aware Attention\": GraphAwareAttentionSummarizer(),\n",
        "    \"KG Consistency Checking\": KGConsistencyCheckingSummarizer(),\n",
        "    ## \"KG Embedding Integration\": KGEmbeddingIntegrationSummarizer() ## deprecated\n",
        "}"
      ],
      "metadata": {
        "id": "ATH1uZIeG4hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move models to the appropriate device\n",
        "for summarizer in summarizers.values():\n",
        "    summarizer.model.to(device)"
      ],
      "metadata": {
        "id": "LHCiPjgd2SWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug: Print first article and summary\n",
        "print(\"\\nFirst article:\")\n",
        "print(dataset[0]['article'])\n",
        "print(\"\\nFirst summary:\")\n",
        "print(dataset[0]['highlights'])"
      ],
      "metadata": {
        "id": "5AwvVPgOMDtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run comparison\n",
        "results, avg_results, example_summaries = compare_summarizers(dataset, summarizers)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HmViKdteJHfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "df_results = pd.DataFrame(avg_results).transpose()\n",
        "print(\"\\nAverage Scores:\")\n",
        "print(df_results)"
      ],
      "metadata": {
        "id": "KHAkXEsy2WkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save example summaries\n",
        "with open(f'example_summaries_{timestamp}.txt', 'w') as f:\n",
        "    for name, summaries in example_summaries.items():\n",
        "        f.write(f\"\\n\\n{name} Summaries:\\n\")\n",
        "        for i, summary in enumerate(summaries):\n",
        "            f.write(f\"\\nExample {i+1}:\\n\")\n",
        "            f.write(f\"Article: {summary['article'][:200]}...\\n\")\n",
        "            f.write(f\"Reference: {summary['reference']}\\n\")\n",
        "            f.write(f\"Generated: {summary['generated']}\\n\")"
      ],
      "metadata": {
        "id": "e13YodHrJVJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log example summaries file to wandb\n",
        "wandb.save(f'example_summaries_{timestamp}.txt')"
      ],
      "metadata": {
        "id": "Qhyg5EzRJXkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to CSV\n",
        "csv_filename = f\"summarizer_comparison_results_{timestamp}.csv\" # unqi\n",
        "df_results.to_csv(csv_filename)\n",
        "print(f\"\\nResults saved to {csv_filename}\")"
      ],
      "metadata": {
        "id": "FosuNP3Y2Wn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log CSV file to wandb\n",
        "wandb.save(csv_filename)"
      ],
      "metadata": {
        "id": "RPt_Na6o2ZXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical Analysis\n",
        "print(\"\\nStatistical Analysis:\")\n",
        "for metric in df_results.columns:\n",
        "    print(f\"\\nMetric: {metric}\")\n",
        "    _, p_value = stats.f_oneway(*(\n",
        "        [scores[metric] for scores in results[name]]\n",
        "        for name in summarizers.keys()\n",
        "    ))\n",
        "    print(f\"One-way ANOVA p-value: {p_value:.4f}\")\n",
        "\n",
        "    # Log p-value to wandb\n",
        "    wandb.log({f\"p_value_{metric}\": p_value})\n",
        "\n",
        "    if p_value < 0.05:\n",
        "        print(\"Significant difference detected. Performing post-hoc Tukey HSD test.\")\n",
        "        tukey_results = stats.tukey_hsd(*(\n",
        "            [scores[metric] for scores in results[name]]\n",
        "            for name in summarizers.keys()\n",
        "        ))\n",
        "        print(tukey_results)\n",
        "\n",
        "        # Log Tukey HSD results to wandb\n",
        "        wandb.log({f\"tukey_hsd_{metric}\": tukey_results})"
      ],
      "metadata": {
        "id": "Y4kzzHc02Zc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizations\n",
        "plt.figure(figsize=(15, 8))  # Increased figure size to accommodate more metrics\n",
        "sns.boxplot(data=pd.melt(pd.DataFrame(results)))\n",
        "plt.title(\"Distribution of Scores Across Metrics and Models\")\n",
        "plt.xlabel(\"Metric\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "XW-PQnSfxyTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and log plot to wandb\n",
        "plot_filename = \"score_distribution.png\"\n",
        "plt.savefig(plot_filename)\n",
        "wandb.log({\"score_distribution\": wandb.Image(plot_filename)})\n",
        "\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "JIm0PJmC2eUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# End the wandb run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "bluWMSu92hP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YMiwOV33HDIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unit tests"
      ],
      "metadata": {
        "id": "Qq75l7yqZKDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KG extraction"
      ],
      "metadata": {
        "id": "ymbwE1l3HH9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class TestKnowledgeGraphExtractor(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.extractor = KnowledgeGraphExtractor()\n",
        "\n",
        "    def test_simple_subject_object_relation(self):\n",
        "        text = \"John gave Mary a book.\"\n",
        "        entities, relations = self.extractor.extract_entities_and_relations(text)\n",
        "        self.assertIn(('John', 'PERSON'), entities)\n",
        "        self.assertIn(('Mary', 'PERSON'), entities)\n",
        "        self.assertIn(('book', 'OBJ'), [ent[0] for ent in entities])\n",
        "        self.assertIn(('John', 'gave', 'book'), relations)\n",
        "\n",
        "    def test_passive_voice_with_agent(self):\n",
        "        text = \"The book was given by John.\"\n",
        "        entities, relations = self.extractor.extract_entities_and_relations(text)\n",
        "        self.assertIn(('John', 'PERSON'), entities)\n",
        "        self.assertIn(('book', 'OBJ'), [ent[0] for ent in entities])\n",
        "        self.assertIn(('book', 'by', 'John'), relations)\n",
        "\n",
        "    def test_prepositional_relation(self):\n",
        "        text = \"Mary put the book on the table.\"\n",
        "        entities, relations = self.extractor.extract_entities_and_relations(text)\n",
        "        self.assertIn(('Mary', 'PERSON'), entities)\n",
        "        self.assertIn(('book', 'OBJ'), [ent[0] for ent in entities])\n",
        "        self.assertIn(('book', 'on', 'table'), relations)\n",
        "\n",
        "    def test_no_relations(self):\n",
        "        text = \"This is a test sentence with no relations.\"\n",
        "        entities, relations = self.extractor.extract_entities_and_relations(text)\n",
        "        self.assertEqual(len(relations), 0)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main()\n"
      ],
      "metadata": {
        "id": "aFO9SqUMZKGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KG consistency checker\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "# Define two knowledge graphs (G1 and G2)\n",
        "G1 = nx.Graph()\n",
        "G1.add_node(\"John\", label=\"PERSON\")\n",
        "G1.add_node(\"Mary\", label=\"PERSON\")\n",
        "G1.add_node(\"book\", label=\"OBJECT\")\n",
        "G1.add_edge(\"John\", \"book\", relation=\"gave\")\n",
        "G1.add_edge(\"book\", \"Mary\", relation=\"to\")\n",
        "\n",
        "G2 = nx.Graph()\n",
        "G2.add_node(\"John\", label=\"PERSON\")\n",
        "G2.add_node(\"Mary\", label=\"PERSON\")\n",
        "G2.add_node(\"book\", label=\"OBJECT\")\n",
        "G2.add_edge(\"John\", \"book\", relation=\"handed\")\n",
        "G2.add_edge(\"book\", \"Mary\", relation=\"to\")\n",
        "\n",
        "# Instantiate and calculate consistency\n",
        "checker = ImprovedConsistencyChecker()\n",
        "consistency_score = checker.calculate_consistency(G1, G2)\n",
        "print(f\"Consistency Score: {consistency_score}\")\n"
      ],
      "metadata": {
        "id": "lk-Vo2kFeybY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h50bjjRvDvx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a KG-enhanced version of CNN/Daily Mail"
      ],
      "metadata": {
        "id": "4GHXZieZDv4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CNN/Daily Mail dataset using the datasets library\n",
        "def load_cnn_dailymail_dataset():\n",
        "    # Load a subset of the dataset containing articles\n",
        "    dataset = load_dataset('cnn_dailymail', '3.0.0', split='train[:100]')  # Load 100 articles as a subset\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "S9h9FZehDv7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process CNN/Daily Mail dataset articles using the KnowledgeGraphGenerator\n",
        "def process_cnn_dailymail_articles(dataset):\n",
        "    kg_generator = KnowledgeGraphGenerator(use_trf_model=True)\n",
        "    kg_data = []\n",
        "\n",
        "    for sample in tqdm(dataset, desc=\"Processing articles\", unit=\"article\"):\n",
        "        article = sample['article']  # Focus on article text\n",
        "        summary = sample.get('highlights', '')  # Extract the reference summary (highlights)\n",
        "\n",
        "        # Create a knowledge graph for the article\n",
        "        G = kg_generator.create_graph(article)\n",
        "        G = kg_generator.clean_knowledge_graph(G)\n",
        "\n",
        "        # Extract and format entities and relations from the graph\n",
        "        entities = list(G.nodes(data=\"label\"))\n",
        "        relations = [(u, v, d['relation']) for u, v, d in G.edges(data=True)]\n",
        "\n",
        "        # Store the article, entities, relations, and summary\n",
        "        kg_data.append({\n",
        "            'article': article,\n",
        "            'entities': [e[0] for e in entities],\n",
        "            'relations': relations,\n",
        "            'summary': summary  # Include the reference summary\n",
        "        })\n",
        "\n",
        "    return kg_data"
      ],
      "metadata": {
        "id": "UoIBfWxwI1OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the KG-enhanced dataset to a CSV file\n",
        "def save_kg_enhanced_articles_dataset(kg_data, output_path):\n",
        "    df_kg = pd.DataFrame(kg_data)\n",
        "    df_kg.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "IqhxtpJLJEew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution flow\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 1: Load the dataset\n",
        "    logging.info(\"Loading CNN/Daily Mail dataset...\")\n",
        "    dataset = load_cnn_dailymail_dataset()\n",
        "    logging.info(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Step 2: Process the CNN/Daily Mail articles to extract entities, relations, and create KG\n",
        "    logging.info(\"Starting to process CNN/Daily Mail articles for KG extraction...\")\n",
        "    kg_data = process_cnn_dailymail_articles(dataset)\n",
        "    logging.info(\"KG extraction completed successfully.\")\n",
        "\n",
        "    # Step 3: Save the KG-enhanced articles dataset to a new CSV file\n",
        "    output_path = 'kg_enhanced_cnn_dailymail_articles.csv'\n",
        "    logging.info(f\"Saving the KG-enhanced articles dataset to {output_path}...\")\n",
        "    save_kg_enhanced_articles_dataset(kg_data, output_path)\n",
        "    logging.info(f\"Dataset saved successfully at {output_path}.\")\n",
        "\n",
        "    # (Optional) Print the first entry of the KG-enhanced data to verify the output\n",
        "    logging.info(\"Printing the first entry of the KG-enhanced data for verification...\")\n",
        "    print(kg_data[0])"
      ],
      "metadata": {
        "id": "Gc5serG8I1SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning Base Bart on KG-enhanced dataset"
      ],
      "metadata": {
        "id": "6SsnmIn-Dv-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize Wandb for logging\n",
        "wandb.init(project=\"KG-enhanced-BART-summarization\", config={\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"batch_size\": 4,\n",
        "    \"epochs\": 3,\n",
        "    \"subset_size\": 100  # Subset size of CNN/Daily Mail for initial fine-tuning\n",
        "})\n",
        "\n",
        "# Step 2: Load KG-Enhanced CNN/Daily Mail Subset (replace with your KG-enhanced CSV file path)\n",
        "kg_dataset_path = '/content/kg_enhanced_cnn_dailymail_articles.csv'\n",
        "df = pd.read_csv(kg_dataset_path)\n",
        "\n",
        "# Step 3: Create a custom dataset class for BART fine-tuning\n",
        "class CNN_DailyMailDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=1024):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.data.iloc[idx]['article']\n",
        "        summary = self.data.iloc[idx]['summary']\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            article, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        labels = self.tokenizer(\n",
        "            summary, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        ).input_ids\n",
        "\n",
        "        inputs[\"labels\"] = labels\n",
        "        return inputs\n",
        "\n",
        "# Evaluation method\n",
        "\n",
        "def evaluate_model_during_training(model, dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Evaluate the fine-tuned model using ROUGE, BLEU, and BERTScore during training.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "        for idx in range(len(dataset)):\n",
        "            article = dataset.iloc[idx]['article']\n",
        "            summary = dataset.iloc[idx]['summary']\n",
        "\n",
        "            inputs = tokenizer(article, max_length=1024, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "            generated_ids = model.generate(inputs, max_length=150, num_beams=5, length_penalty=2.0)\n",
        "            prediction = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "            predictions.append(prediction)\n",
        "            references.append(summary)\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = [rouge_scorer_instance.score(ref, pred) for ref, pred in zip(references, predictions)]\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
        "\n",
        "    # Calculate BERTScore\n",
        "    bertscore = bert_score.score(predictions, references, lang=\"en\")\n",
        "\n",
        "    return rouge_scores, bleu.score, bertscore\n",
        "\n",
        "# Step 4: Initialize BART tokenizer and model\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").cuda()\n",
        "\n",
        "# Step 5: Prepare the dataset and dataloader\n",
        "dataset = CNN_DailyMailDataset(df, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
        "\n",
        "# Step 6: Setup optimizer, scaler (for AMP), and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=wandb.config.learning_rate)\n",
        "scaler = GradScaler()\n",
        "scheduler = get_scheduler(\n",
        "    \"cosine\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=len(dataloader) * wandb.config.epochs\n",
        ")\n",
        "\n",
        "# Step 7: Fine-tuning loop with AMP and gradient accumulation\n",
        "for epoch in range(wandb.config.epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        batch = {key: value.squeeze().cuda() for key, value in batch.items()}  # Move the batch to GPU\n",
        "\n",
        "        with autocast():  # Automatic Mixed Precision for faster training\n",
        "            outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"labels\"])\n",
        "            loss = outputs.loss  # Loss per batch\n",
        "\n",
        "        scaler.scale(loss).backward()  # Scale the loss for mixed precision\n",
        "\n",
        "        if (i + 1) % wandb.config.batch_size == 0:  # Perform optimizer step after accumulating gradients\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()  # Clear gradients after step\n",
        "            scheduler.step()  # Update the learning rate based on scheduler\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        wandb.log({\"training_loss\": total_loss / (i + 1)})  # Log the average loss up to this point\n",
        "\n",
        "    # Log epoch-wise metrics and save the model at the end of each epoch\n",
        "    model.save_pretrained(f\"bart_finetuned_epoch_{epoch}\")\n",
        "    wandb.log({\"epoch\": epoch, \"avg_loss\": total_loss / len(dataloader)})\n",
        "\n",
        "    # **Evaluate the model after each epoch**\n",
        "    rouge_scores, bleu_score_value, bertscore = evaluate_model_during_training(model, df, tokenizer)\n",
        "\n",
        "    # **Log evaluation metrics to W&B**\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"ROUGE-1\": sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "        \"ROUGE-2\": sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "        \"ROUGE-L\": sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "        \"BLEU\": bleu_score_value,\n",
        "        \"BERTScore_Precision\": bertscore[0].mean().item(),\n",
        "        \"BERTScore_Recall\": bertscore[1].mean().item(),\n",
        "        \"BERTScore_F1\": bertscore[2].mean().item()\n",
        "    })\n",
        "\n",
        "# Finalize logging after all epochs are done\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "YzwbEw3JQDoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-rfxXVOZIBBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mjSoqDCKIBEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6KFA0AlIBHZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}